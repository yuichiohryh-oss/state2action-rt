# PROJECT_STATE.md

## 目的

本プロジェクトは、リアルタイム対戦ゲームのプレイ画面（動画）から
**「次に取るべき行動を提案する AI」** を構築することを目的とする。

* 自動操作ではなく **人間への提案（Top-k）** が主目的
* 教師あり学習を基本とし、段階的に高度化する
* repo 内部では実名カード名を使用し、公開記事では一般名に一般化する

---

## 全体構成（俯瞰）

```
動画 / スクリーン
   ↓
[前処理]
  - 盤面 ROI 切り出し
  - フレーム整列（current / past / diff）
  - 手札 availability 抽出（予定）
   ↓
入力テンソル [9,256,256] + 数値特徴
   ↓
CNN（将来 Transformer へ拡張可能）
   ↓
2ヘッド出力
  - action head
  - grid head
   ↓
Top-k 行動提案
```

---

## 入力設計（確定）

### 盤面画像入力

* 盤面 ROI を 256×256 に正規化
* 入力テンソル shape: **[9, 256, 256]**

| チャンネル | 内容                         |
| ----- | -------------------------- |
| 0–2   | current frame（RGB）         |
| 3–5   | past frame（RGB）            |
| 6–8   | diff = current − past（RGB） |

* 2フレーム入力 + 差分チャンネル
* この入力 IF は将来も固定（CNN / Transformer 差し替え前提）

---

## 盤面 ROI（確定ベースライン）

* 対象：720p 縦長動画（YouTube / scrcpy）
* 実機確認済みパラメータ

```
--roi-y1 110
--roi-y2-mode fixed
--roi-y2-fixed 570
--gw 6 --gh 9
--lead-sec 0.8
```

* 上下 UI を除外し、盤面のみを安定して取得
* 動画レイアウトが変わる場合は `inspect_sample.py` で必ず目視確認

---

## 出力設計（確定）

### 2ヘッド構成

1. **action head**

   * 行動の種類を分類
   * カード / スキル / NOOP

2. **grid head**

   * 盤面上の配置位置（6×9）

---

## 行動実行フロー（重要・確定）

本プロジェクトでは、ゲームの操作仕様を厳密に反映した **段階的実行フロー** を前提とする。

### カード行動（2段階）

カードを盤面に出すには、必ず以下の 2 ステップを踏む必要がある。

1. **手札選択**

   * 画面下部の手札 UI から、出したいカードを選択
   * 手札は常に 4 枠表示（順序はランダム）

2. **盤面配置**

   * 選択後、盤面上の任意の位置（grid）をタップ
   * その位置にユニット／効果が出現

このため、自動実行では以下が必要となる。

* 推論結果 `action_id` が **現在の手札 4 枠のどこに存在するか（slot index）** の特定
* 手札に存在しない場合は **実行しない（安全側）**

### スキル行動（1段階）

* チャンピオンスキルは **スキルボタンを押すだけで即時発動**
* 盤面位置の選択は不要
* 以下の場合は使用不可とする

  * ボタンがグレー表示
  * ボタンが存在しない（ヒーロー未配置）

### NOOP（0段階）

* 何も操作を行わない
* 行動としては有効な判断
* ログのみ記録

---

## アクション定義ポリシー

* デッキ固定（例：8枚） + SKILL_RIGHT + NOOP
* repo 内部では **実名カード名** を使用
* dataset.jsonl では action_id を文字列で保持
* モデル内部で index に変換

---

## 学習方針

* 教師あり学習
* 出力は Top-k 提案
* 人間が最終判断を行う

### NOOP の扱い

* 実データでは NOOP が多くなりがち
* `augment_noop.py` により NOOP を補完生成
* loss mask / weight により NOOP 過多を防止

---

## 行動制約（重要）

### 問題

* 同一盤面でも、

  * エリクサー不足
  * 手札状態
    により **取れる行動が変わる**

→ 盤面のみを入力とすると、教師信号が矛盾する

---

## 手札 availability（導入確定・最優先）

### 方針

* エリクサー量の数値推定よりも先に導入
* UI 表示（カラー / グレー）を直接利用

### 実装仕様

* 手札 ROI：画面下端 **90–97%**
* 横方向に 4 分割（4 スロット）
* HSV 色空間で **彩度（S）平均**を算出

| 判定   | 条件           |
| ---- | ------------ |
| 出せる  | mean_S > 約30 |
| 出せない | mean_S ≤ 約30 |

### データ表現

```json
"hand_available": [0, 0, 1, 1]
```

* 学習時：数値特徴量として使用
* 推論時：不可能な行動を抑制するマスクに使用

---

## サイクル読み・Transformer に関する整理

* 相手カード完全認識によるサイクル把握は難易度が高い
* 初期フェーズでは **盤面 + diff** に集中
* 将来、短い履歴（イベント列）に対して Transformer / Attention を導入
* 入出力 IF（[9,256,256] → action, grid）は変更しない

---

## 開発フェーズ

### フェーズ①

* YouTube プロ試合（約20試合）から初期モデル M1 を作成

### フェーズ②（現在）

* M1 を使って自分がプレイ（scrcpy 720p）
* 動画 + クリックログを取得
* 後からラベリング
* M1 の Top-k 提案をラベラーに表示（予定）

### フェーズ③

* 自分のプレイログを再学習
* ②③を反復して強化

---

## 追加特徴量の優先度（最新版）

本プロジェクトでは、人間の判断に近づけるための追加特徴量を段階的に導入する。
実装コストと学習効果を踏まえた **優先度の最終整理** は以下の通り。

### 確定（優先度：高）

1. **card_ready（手札 availability）**

   * 各カードが「今出せるか」を 0/1 で表現
   * UI のカラー／グレー判定を利用
   * 行動制約として最重要

2. **skill_ready_right**

   * 右側チャンピオンスキルが使用可能か（0/1）
   * カラーボタン＝可、グレー／ボタンなし＝不可

3. **elixir_ratio**

   * エリクサーバーの塗り割合から推定した 0.0–1.0 の連続値
   * 人間の「余裕」「攻守判断」を反映
   * まずは粗い推定で十分

4. **remaining_ratio**

   * 残り時間を試合全体（例：180秒）で正規化した値
   * OCR は使わず、動画の経過時間から算出

### 将来導入（優先度：中〜低）

5. **next_card**

   * 次に回ってくるカード情報
   * サイクル読み精度向上に寄与

6. **tower_hp**

   * タワー耐久値（数値または粗いカテゴリ）
   * 攻守判断の高度化に有効

---

## 次の具体タスク

## 設計思想（重要）

* **入出力 IF を固定し、内部モデルを差し替え可能にする**
* 盤面理解（CNN）と制約理解（数値特徴）を分離
* 完全自動化より、人間との協調を重視

---

## ゴール像

* プロの判断を高頻度で再現する提案 AI
* 単独でプロに勝つことより
  **「プロと組んで戦う感覚」** を目指す

## カード番号

```
templates/hand_cards/
  0_cannon.png
  1_cannon_evo.png
  2_fireball.png
  3_hog.png
  4_icegolem.png
  5_ice_spirit.png
  6_log.png
  7_musketeer.png
  8_skeletons.png
  9_skeletons_evo.png
```
